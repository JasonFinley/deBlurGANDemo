{"cells":[{"cell_type":"markdown","metadata":{"id":"WLy3osPTuJPY"},"source":["# Deblur-V2 圖像去模糊模型\n"]},{"cell_type":"markdown","source":["## **Change history**\n","\n"],"metadata":{"id":"Oa-AxwVh1pgQ"}},{"cell_type":"code","source":["##change date : 2025/10/21 00:00-JASON\n","# 加G-LOSS/D-LOSS/PSNR/SSIM 每個ephoch 的平均值\n","# 加PSNR/SSIM Rescale圖形\n","#===========================================\n","#change date : 2025/10/20 14:00\n","#add 抓儲存的訓練檔案（checkpoint）中載入模型權重與紀錄的訓練數據（loss、PSNR、SSIM）的最後10筆\n","# ##code change part==>title: Load Loss Metric\n","\n","#===========================================\n","\n","#Change History :Modify DATE:1020 10:00am\n","#調整LOSS權重\n","#lambda_L1    = 1.5 from 1.0    # L1 重建 loss 權重\n","#lambda_perc  = 0.10 from 0.05      # perceptual loss 權重;\n","#lambda_gan   = 1e-4 from 5e-3     # adv loss 權重#default\n","#num_epochs = 75\n","\n"],"metadata":{"id":"dsFkIRZa4OrG","executionInfo":{"status":"ok","timestamp":1761036970803,"user_tz":-480,"elapsed":5,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_ahc0WRuJPa"},"source":["## 步驟 1: 安裝依賴包\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32503,"status":"ok","timestamp":1761037003336,"user":{"displayName":"Rosalind","userId":"11181186080888946070"},"user_tz":-480},"id":"5I8CRVHtOTCk","outputId":"27283436-34ea-4543-b500-7e586cd8217b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n","Collecting pretrainedmodels\n","  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/58.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Collecting munch (from pretrainedmodels)\n","  Downloading munch-4.0.0-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n","Downloading munch-4.0.0-py2.py3-none-any.whl (9.9 kB)\n","Building wheels for collected packages: pretrainedmodels\n","  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60945 sha256=f873073af90438895c2ec5d74a977da4a23d27c639d77b9c2a362d1dfb54d5ee\n","  Stored in directory: /root/.cache/pip/wheels/4c/01/56/40a48f75dbdfe167a0cb70d3b48913369a00ec5c4e9fed5f2b\n","Successfully built pretrainedmodels\n","Installing collected packages: munch, pretrainedmodels\n","Successfully installed munch-4.0.0 pretrainedmodels-0.7.4\n","Collecting pytorch-msssim\n","  Downloading pytorch_msssim-1.0.0-py3-none-any.whl.metadata (8.0 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from pytorch-msssim) (2.8.0+cu126)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->pytorch-msssim) (3.4.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->pytorch-msssim) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->pytorch-msssim) (3.0.3)\n","Downloading pytorch_msssim-1.0.0-py3-none-any.whl (7.7 kB)\n","Installing collected packages: pytorch-msssim\n","Successfully installed pytorch-msssim-1.0.0\n"]}],"source":["# 使用 pip 安裝所需的 Python 套件\n","!pip install torch torchvision tqdm pillow pretrainedmodels\n","!pip install pytorch-msssim\n","\n","# 匯入作業系統相關功能 (例如：讀取檔案路徑)\n","import os\n","\n","# 匯入 PyTorch 主套件\n","import torch\n","\n","# 匯入 PyTorch 的神經網路模組 (用來建立模型)\n","import torch.nn as nn\n","\n","# 匯入 PyTorch 的優化器模組 (用來訓練模型)\n","import torch.optim as optim\n","\n","# 從 pretrainedmodels 套件中匯入 inceptionresnetv2 模型 (一種預訓練好的 CNN 模型)\n","from pretrainedmodels import inceptionresnetv2\n","\n","# 匯入 PyTorch 的 DataLoader 和 Dataset (用來處理資料批次與自訂資料集)\n","from torch.utils.data import DataLoader, Dataset\n","\n","# 匯入 torchvision 的工具 (用來進行圖片轉換與使用其他預訓練模型)\n","from torchvision import transforms, models\n","\n","# 從 PIL (Python Imaging Library) 匯入 Image 類別，用來開啟與處理圖片\n","from PIL import Image\n","\n","# 匯入 tqdm 套件，用來顯示進度條\n","from tqdm import tqdm\n","\n","# 匯入 matplotlib 套件的 pyplot 模組，用來畫圖或顯示結果\n","import matplotlib.pyplot as plt\n","\n","# 檢查是否有可用的 GPU，如果有就使用 CUDA，否則使用 CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"RJJPY_MseJEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761037036390,"user_tz":-480,"elapsed":33011,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}},"outputId":"8ccc99e4-a250-47fd-95c6-2db8ba93194b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"hzVdii8cuJPd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761037069159,"user_tz":-480,"elapsed":50,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}},"outputId":"40cab7f3-d41b-4eb4-801e-f5a08259b8a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["✓ 檢查點目錄: /content/drive/MyDrive/Colab Notebooks/ModelClearify/checkpoints\n"]}],"source":["checkpoint_dir = \"/content/drive/MyDrive/Colab Notebooks/ModelClearify/checkpoints\"\n","\n","### 設置數據集路徑\n","#defocused_dir = '/content/drive/MyDrive/Colab Notebooks/datasets/kaggle_blur_dataset/kaggle/defocused_blurred'\n","#motion_dir = '/content/drive/MyDrive/Colab Notebooks/datasets/kaggle_blur_dataset/kaggle/motion_blurred'\n","#sharp_dir = '/content/drive/MyDrive/Colab Notebooks/datasets/kaggle_blur_dataset/kaggle/sharp'\n","\n","### GoPro組訓練圖 - 696張\n","motion_dir = '/content/drive/MyDrive/Colab Notebooks/datasets/kaggle_blur_dataset/GoPro/motion_blur'\n","sharp_dir = '/content/drive/MyDrive/Colab Notebooks/datasets/kaggle_blur_dataset/GoPro/sharp'\n","\n","# 創建檢查點目錄\n","os.makedirs(checkpoint_dir, exist_ok=True)\n","print(f\"✓ 檢查點目錄: {checkpoint_dir}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"xuPuEl-u4Ak1","executionInfo":{"status":"ok","timestamp":1761037073236,"user_tz":-480,"elapsed":5,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["import random\n","import numpy as np\n","import torch\n","from torch.autograd import Variable\n","from collections import deque\n","\n","\n","class ImagePool():\n","    def __init__(self, pool_size):\n","        self.pool_size = pool_size\n","        self.sample_size = pool_size\n","        if self.pool_size > 0:\n","            self.num_imgs = 0\n","            self.images = deque()\n","\n","    def add(self, images):\n","        if self.pool_size == 0:\n","            return images\n","        for image in images.data:\n","            image = torch.unsqueeze(image, 0)\n","            if self.num_imgs < self.pool_size:\n","                self.num_imgs = self.num_imgs + 1\n","                self.images.append(image)\n","            else:\n","                self.images.popleft()\n","                self.images.append(image)\n","\n","    def query(self):\n","        if len(self.images) > self.sample_size:\n","            return_images = list(random.sample(self.images, self.sample_size))\n","        else:\n","            return_images = list(self.images)\n","        return torch.cat(return_images, 0)\n"]},{"cell_type":"markdown","metadata":{"id":"6DCpX5lmObks"},"source":["## 資料"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Gv_WnM5OOpCD","executionInfo":{"status":"ok","timestamp":1761037075750,"user_tz":-480,"elapsed":4,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["class DeblurDataset(Dataset):\n","    def __init__(self, sharp_dir, blur_dir, transform=None, patch_size=512, random_crop_cnt=10):\n","        self.sharp_dir = sharp_dir\n","        self.blur_dir = blur_dir\n","        self.transform = transform\n","        self.random_crop_cnt = random_crop_cnt\n","        self.patch_size = patch_size\n","\n","        blur_files = sorted(os.listdir(blur_dir))\n","        sharp_files = sorted(os.listdir(sharp_dir))\n","\n","        def get_id(fname):\n","            return fname.split('_')[0]\n","\n","        blur_dict = {get_id(f): f for f in blur_files}\n","        sharp_dict = {get_id(f): f for f in sharp_files}\n","\n","        self.paired_keys = sorted(list(set(blur_dict.keys()) & set(sharp_dict.keys())))\n","        self.blur_dict = blur_dict\n","        self.sharp_dict = sharp_dict\n","\n","        self.total_patches = len(self.paired_keys)\n","        self.to_tensor = transforms.ToTensor()\n","\n","    def __len__(self):\n","        return self.total_patches * self.random_crop_cnt\n","\n","    def __getitem__(self, idx):\n","        img_idx = idx % self.total_patches\n","        key = self.paired_keys[img_idx]\n","\n","        blur_path = os.path.join(self.blur_dir, self.blur_dict[key])\n","        sharp_path = os.path.join(self.sharp_dir, self.sharp_dict[key])\n","\n","        blur_img = Image.open(blur_path).convert(\"RGB\")\n","        sharp_img = Image.open(sharp_path).convert(\"RGB\")\n","        # 確認兩張圖尺寸相同\n","        if blur_img.size != sharp_img.size:\n","            raise ValueError(f\"Size mismatch: {blur_path} vs {sharp_path}\")\n","\n","        # --- 尺寸檢查與修正成 32 的倍數 ---\n","        w, h = blur_img.size\n","        new_w = (w // 32) * 32\n","        new_h = (h // 32) * 32\n","        if new_w != w or new_h != h:\n","            # print(f\"Resizing {key} from ({w},{h}) → ({new_w},{new_h})\")\n","            blur_img = blur_img.resize((new_w, new_h), Image.BICUBIC)\n","            sharp_img = sharp_img.resize((new_w, new_h), Image.BICUBIC)\n","            w, h = new_w, new_h\n","\n","        # --- 裁切 patch ---\n","        ps = self.patch_size\n","        if w > ps and h > ps:\n","            if self.random_crop_cnt > 1:\n","                x = random.randint(0, w - ps)\n","                y = random.randint(0, h - ps)\n","            else:\n","                x = (w - ps) // 2\n","                y = (h - ps) // 2\n","            blur_img = blur_img.crop((x, y, x + ps, y + ps))\n","            sharp_img = sharp_img.crop((x, y, x + ps, y + ps))\n","        else:\n","            # 若比 patch 小，放大成指定大小\n","            blur_img = blur_img.resize((ps, ps), Image.BICUBIC)\n","            sharp_img = sharp_img.resize((ps, ps), Image.BICUBIC)\n","\n","        if self.transform:\n","          sharp_img = self.transform(sharp_img)\n","          blur_img = self.transform(blur_img)\n","\n","        return blur_img, sharp_img, key  # (input, target)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"0xXxTOeZOxO0","executionInfo":{"status":"ok","timestamp":1761037084463,"user_tz":-480,"elapsed":5097,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))\n","])\n","\n","dataset = DeblurDataset(sharp_dir, motion_dir, transform=transform)\n","loader = DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2)\n","\n","# check dateset\n","# tmp_bar = tqdm(loader, desc=f\"Epoch {1}/{1}\")\n","# for i, (blur_img, sharp_img, _) in enumerate(tmp_bar):\n","#  print(f\"\\nblur shape: {tuple(blur_img.shape)}, sharp shape: {tuple(sharp_img.shape)}\")\n","#  break  # 只印第一筆可以加快測試"]},{"cell_type":"markdown","metadata":{"id":"LTveVlwHuJPd"},"source":["## 模型構建\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jejw_n3JP9BJ","executionInfo":{"status":"ok","timestamp":1761037084496,"user_tz":-480,"elapsed":29,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchsummary import summary\n","import torch.nn.functional as F\n","\n","class FPNHead(nn.Module):\n","    def __init__(self, num_in, num_mid, num_out):\n","        super().__init__()\n","\n","        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n","        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.block0(x), inplace=True)\n","        x = nn.functional.relu(self.block1(x), inplace=True)\n","        return x\n","\n","class ConvBlock(nn.Module):\n","    def __init__(self, num_in, num_out, norm_layer):\n","        super().__init__()\n","\n","        self.block = nn.Sequential(nn.Conv2d(num_in, num_out, kernel_size=3, padding=1),\n","                                 norm_layer(num_out),\n","                                 nn.ReLU(inplace=True))\n","\n","    def forward(self, x):\n","        x = self.block(x)\n","        return x\n","\n","\n","class FPNInception(nn.Module):\n","\n","    def __init__(self, norm_layer=nn.InstanceNorm2d, output_ch=3, num_filters=128, num_filters_fpn=256):\n","        super().__init__()\n","\n","        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n","        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n","        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer=norm_layer)\n","\n","        # The segmentation heads on top of the FPN\n","\n","        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n","        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n","        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n","        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n","\n","        self.smooth = nn.Sequential(\n","            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n","            norm_layer(num_filters),\n","            nn.ReLU(),\n","        )\n","\n","        self.smooth2 = nn.Sequential(\n","            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n","            norm_layer(num_filters // 2),\n","            nn.ReLU(),\n","        )\n","\n","        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n","\n","    def unfreeze(self):\n","        self.fpn.unfreeze()\n","\n","    def forward(self, x):\n","        map0, map1, map2, map3, map4 = self.fpn(x)\n","\n","        map4 = nn.functional.interpolate(self.head4(map4), scale_factor=8, mode=\"nearest\")\n","        map3 = nn.functional.interpolate(self.head3(map3), scale_factor=4, mode=\"nearest\")\n","        map2 = nn.functional.interpolate(self.head2(map2), scale_factor=2, mode=\"nearest\")\n","        map1 = nn.functional.interpolate(self.head1(map1), scale_factor=1, mode=\"nearest\")\n","\n","        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n","        smoothed = nn.functional.interpolate(smoothed, scale_factor=2, mode=\"nearest\")\n","        smoothed = self.smooth2(smoothed + map0)\n","        smoothed = nn.functional.interpolate(smoothed, scale_factor=2, mode=\"nearest\")\n","\n","        final = self.final(smoothed)\n","        res = torch.tanh(final) + x\n","\n","        return torch.clamp(res, min = -1,max = 1)\n","\n","\n","class FPN(nn.Module):\n","\n","    def __init__(self, norm_layer, num_filters=256):\n","        \"\"\"Creates an `FPN` instance for feature extraction.\n","        Args:\n","          num_filters: the number of filters in each output pyramid level\n","          pretrained: use ImageNet pre-trained backbone feature extractor\n","        \"\"\"\n","\n","        super().__init__()\n","        self.inception = inceptionresnetv2(num_classes=1000, pretrained='imagenet')\n","\n","        self.enc0 = self.inception.conv2d_1a\n","        self.enc1 = nn.Sequential(\n","            self.inception.conv2d_2a,\n","            self.inception.conv2d_2b,\n","            self.inception.maxpool_3a,\n","        ) # 64\n","        self.enc2 = nn.Sequential(\n","            self.inception.conv2d_3b,\n","            self.inception.conv2d_4a,\n","            self.inception.maxpool_5a,\n","        )  # 192\n","        self.enc3 = nn.Sequential(\n","            self.inception.mixed_5b,\n","            self.inception.repeat,\n","            self.inception.mixed_6a,\n","        )   # 1088\n","        self.enc4 = nn.Sequential(\n","            self.inception.repeat_1,\n","            self.inception.mixed_7a,\n","        ) #2080\n","        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n","                                 norm_layer(num_filters),\n","                                 nn.ReLU(inplace=True))\n","        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n","                                 norm_layer(num_filters),\n","                                 nn.ReLU(inplace=True))\n","        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n","                                 norm_layer(num_filters),\n","                                 nn.ReLU(inplace=True))\n","        self.pad = nn.ReflectionPad2d(1)\n","        self.lateral4 = nn.Conv2d(2080, num_filters, kernel_size=1, bias=False)\n","        self.lateral3 = nn.Conv2d(1088, num_filters, kernel_size=1, bias=False)\n","        self.lateral2 = nn.Conv2d(192, num_filters, kernel_size=1, bias=False)\n","        self.lateral1 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n","        self.lateral0 = nn.Conv2d(32, num_filters // 2, kernel_size=1, bias=False)\n","\n","        for param in self.inception.parameters():\n","            param.requires_grad = False\n","\n","    def unfreeze(self):\n","        for param in self.inception.parameters():\n","            param.requires_grad = True\n","\n","    def forward(self, x):\n","\n","        # Bottom-up pathway, from ResNet\n","        enc0 = self.enc0(x)\n","\n","        enc1 = self.enc1(enc0) # 256\n","\n","        enc2 = self.enc2(enc1) # 512\n","\n","        enc3 = self.enc3(enc2) # 1024\n","\n","        enc4 = self.enc4(enc3) # 2048\n","\n","        # Lateral connections\n","\n","        lateral4 = self.pad(self.lateral4(enc4))\n","        lateral3 = self.pad(self.lateral3(enc3))\n","        lateral2 = self.lateral2(enc2)\n","        lateral1 = self.pad(self.lateral1(enc1))\n","        lateral0 = self.lateral0(enc0)\n","\n","        # Top-down pathway\n","        pad = (1, 2, 1, 2)  # pad last dim by 1 on each side\n","        pad1 = (0, 1, 0, 1)\n","        map4 = lateral4\n","        map3 = self.td1(lateral3 + nn.functional.interpolate(map4, scale_factor=2, mode=\"nearest\"))\n","        map2 = self.td2(F.pad(lateral2, pad, \"reflect\") + nn.functional.interpolate(map3, scale_factor=2, mode=\"nearest\"))\n","        map1 = self.td3(lateral1 + nn.functional.interpolate(map2, scale_factor=2, mode=\"nearest\"))\n","        return F.pad(lateral0, pad1, \"reflect\"), map1, map2, map3, map4"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"UJITM7AXQoRv","executionInfo":{"status":"ok","timestamp":1761037087147,"user_tz":-480,"elapsed":45,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import init\n","import functools\n","from torch.autograd import Variable\n","import numpy as np\n","###############################################################################\n","# Functions\n","###############################################################################\n","\n","\n","def get_norm_layer(norm_type='instance'):\n","    if norm_type == 'batch':\n","        norm_layer = functools.partial(nn.BatchNorm2d, affine=True)\n","    elif norm_type == 'instance':\n","        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=True)\n","    else:\n","        raise NotImplementedError('normalization layer [%s] is not found' % norm_type)\n","    return norm_layer\n","\n","##############################################################################\n","# Classes\n","##############################################################################\n","\n","\n","# Defines the generator that consists of Resnet blocks between a few\n","# downsampling/upsampling operations.\n","# Code and idea originally from Justin Johnson's architecture.\n","# https://github.com/jcjohnson/fast-neural-style/\n","class ResnetGenerator(nn.Module):\n","    def __init__(self, input_nc=3, output_nc=3, ngf=64, norm_layer=nn.BatchNorm2d, use_dropout=False, n_blocks=6, use_parallel=True, learn_residual=True, padding_type='reflect'):\n","        assert(n_blocks >= 0)\n","        super(ResnetGenerator, self).__init__()\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.ngf = ngf\n","        self.use_parallel = use_parallel\n","        self.learn_residual = learn_residual\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        model = [nn.ReflectionPad2d(3),\n","                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0,\n","                           bias=use_bias),\n","                 norm_layer(ngf),\n","                 nn.ReLU(True)]\n","\n","        n_downsampling = 2\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n","                                stride=2, padding=1, bias=use_bias),\n","                      norm_layer(ngf * mult * 2),\n","                      nn.ReLU(True)]\n","\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            model += [ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, use_dropout=use_dropout, use_bias=use_bias)]\n","\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n","                                         kernel_size=3, stride=2,\n","                                         padding=1, output_padding=1,\n","                                         bias=use_bias),\n","                      norm_layer(int(ngf * mult / 2)),\n","                      nn.ReLU(True)]\n","        model += [nn.ReflectionPad2d(3)]\n","        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n","        model += [nn.Tanh()]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input):\n","        output = self.model(input)\n","        if self.learn_residual:\n","            output = input + output\n","            output = torch.clamp(output,min = -1,max = 1)\n","        return output\n","\n","\n","# Define a resnet block\n","class ResnetBlock(nn.Module):\n","    def __init__(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n","        super(ResnetBlock, self).__init__()\n","        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, use_dropout, use_bias)\n","\n","    def build_conv_block(self, dim, padding_type, norm_layer, use_dropout, use_bias):\n","        conv_block = []\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n","                       norm_layer(dim),\n","                       nn.ReLU(True)]\n","        if use_dropout:\n","            conv_block += [nn.Dropout(0.5)]\n","\n","        p = 0\n","        if padding_type == 'reflect':\n","            conv_block += [nn.ReflectionPad2d(1)]\n","        elif padding_type == 'replicate':\n","            conv_block += [nn.ReplicationPad2d(1)]\n","        elif padding_type == 'zero':\n","            p = 1\n","        else:\n","            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias),\n","                       norm_layer(dim)]\n","\n","        return nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x)\n","        return out\n","\n","\n","class DicsriminatorTail(nn.Module):\n","    def __init__(self, nf_mult, n_layers, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n","        super(DicsriminatorTail, self).__init__()\n","        self.use_parallel = use_parallel\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = int(np.ceil((kw-1)/2))\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2**n_layers, 8)\n","        sequence = [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","\n","class MultiScaleDiscriminator(nn.Module):\n","    def __init__(self, input_nc=3, ndf=64, norm_layer=nn.BatchNorm2d, use_parallel=True):\n","        super(MultiScaleDiscriminator, self).__init__()\n","        self.use_parallel = use_parallel\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = int(np.ceil((kw-1)/2))\n","        sequence = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf_mult = 1\n","        for n in range(1, 3):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2**n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        self.scale_one = nn.Sequential(*sequence)\n","        self.first_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=3)\n","        nf_mult_prev = 4\n","        nf_mult = 8\n","\n","        self.scale_two = nn.Sequential(\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                      kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True))\n","        nf_mult_prev = nf_mult\n","        self.second_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=4)\n","        self.scale_three = nn.Sequential(\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True))\n","        self.third_tail = DicsriminatorTail(nf_mult=nf_mult, n_layers=5)\n","\n","    def forward(self, input):\n","        x = self.scale_one(input)\n","        x_1 = self.first_tail(x)\n","        x = self.scale_two(x)\n","        x_2 = self.second_tail(x)\n","        x = self.scale_three(x)\n","        x = self.third_tail(x)\n","        return [x_1, x_2, x]\n","\n","\n","# Defines the PatchGAN discriminator with the specified arguments.\n","class NLayerDiscriminator(nn.Module):\n","    def __init__(self, input_nc=3, ndf=64, n_layers=3, norm_layer=nn.InstanceNorm2d, use_sigmoid=False, use_parallel=True):\n","        super(NLayerDiscriminator, self).__init__()\n","        self.use_parallel = use_parallel\n","        if type(norm_layer) == functools.partial:\n","            use_bias = norm_layer.func == nn.InstanceNorm2d\n","        else:\n","            use_bias = norm_layer == nn.InstanceNorm2d\n","\n","        kw = 4\n","        padw = int(np.ceil((kw-1)/2))\n","        sequence = [\n","            nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        nf_mult = 1\n","        for n in range(1, n_layers):\n","            nf_mult_prev = nf_mult\n","            nf_mult = min(2**n, 8)\n","            sequence += [\n","                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                          kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n","                norm_layer(ndf * nf_mult),\n","                nn.LeakyReLU(0.2, True)\n","            ]\n","\n","        nf_mult_prev = nf_mult\n","        nf_mult = min(2**n_layers, 8)\n","        sequence += [\n","            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n","                      kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n","            norm_layer(ndf * nf_mult),\n","            nn.LeakyReLU(0.2, True)\n","        ]\n","\n","        sequence += [nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]\n","\n","        if use_sigmoid:\n","            sequence += [nn.Sigmoid()]\n","\n","        self.model = nn.Sequential(*sequence)\n","\n","    def forward(self, input):\n","        return self.model(input)\n","\n","\n","def get_fullD(model_config):\n","    model_d = NLayerDiscriminator(n_layers=5,\n","                                  norm_layer=get_norm_layer(norm_type=model_config['norm_layer']),\n","                                  use_sigmoid=False)\n","    return model_d\n","\n","\n","def get_generator(model_config):\n","    generator_name = model_config['g_name']\n","    if generator_name == 'resnet':\n","        model_g = ResnetGenerator(norm_layer=get_norm_layer(norm_type=model_config['norm_layer']),\n","                                  use_dropout=model_config['dropout'],\n","                                  n_blocks=model_config['blocks'],\n","                                  learn_residual=model_config['learn_residual'])\n","    elif generator_name == 'fpn_inception':\n","        model_g = FPNInception(norm_layer=get_norm_layer(norm_type=model_config['norm_layer']))\n","    elif generator_name == 'fpn_inception_simple':\n","        model_g = FPN(norm_layer=get_norm_layer(norm_type=model_config['norm_layer']))\n","    else:\n","        raise ValueError(\"Generator Network [%s] not recognized.\" % generator_name)\n","\n","    return nn.DataParallel(model_g)\n","\n","\n","def get_discriminator(model_config):\n","    discriminator_name = model_config['d_name']\n","    if discriminator_name == 'no_gan':\n","        model_d = None\n","    elif discriminator_name == 'patch_gan':\n","        model_d = NLayerDiscriminator(n_layers=model_config['d_layers'],\n","                                      norm_layer=get_norm_layer(norm_type=model_config['norm_layer']),\n","                                      use_sigmoid=False)\n","        model_d = nn.DataParallel(model_d)\n","    elif discriminator_name == 'double_gan':\n","        patch_gan = NLayerDiscriminator(n_layers=model_config['d_layers'],\n","                                        norm_layer=get_norm_layer(norm_type=model_config['norm_layer']),\n","                                        use_sigmoid=False)\n","        patch_gan = nn.DataParallel(patch_gan)\n","        full_gan = get_fullD(model_config)\n","        full_gan = nn.DataParallel(full_gan)\n","        model_d = {'patch': patch_gan,\n","                   'full': full_gan}\n","    elif discriminator_name == 'multi_scale':\n","        model_d = MultiScaleDiscriminator(norm_layer=get_norm_layer(norm_type=model_config['norm_layer']))\n","        model_d = nn.DataParallel(model_d)\n","    else:\n","        raise ValueError(\"Discriminator Network [%s] not recognized.\" % discriminator_name)\n","\n","    return model_d\n","\n","\n","def get_nets(model_config):\n","    return get_generator(model_config), get_discriminator(model_config)"]},{"cell_type":"markdown","metadata":{"id":"MFmLfw_iuJPd"},"source":["## Loss Function\n"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"AaGTPvJBSRCm","executionInfo":{"status":"ok","timestamp":1761037098914,"user_tz":-480,"elapsed":10,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["import torch\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","from torch.autograd import Variable\n","\n","###############################################################################\n","# Functions\n","###############################################################################\n","\n","class ContentLoss():\n","    def initialize(self, loss):\n","        self.criterion = loss\n","\n","    def get_loss(self, fakeIm, realIm):\n","        return self.criterion(fakeIm, realIm)\n","\n","    def __call__(self, fakeIm, realIm):\n","        return self.get_loss(fakeIm, realIm)\n","\n","\n","class PerceptualLoss(nn.Module):\n","    def __init__(self, device=None):\n","        super().__init__()\n","        # 如果没有传 device，就自动判断\n","        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.criterion = None\n","        self.contentFunc = None\n","        # transform 用于对输入做归一化（在 feature 提取前）\n","        self.transform = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                                              std=[0.229, 0.224, 0.225])\n","\n","    def contentFunc_builder(self):\n","        \"\"\"构造 VGG 的 feature 提取网络（到 conv_3_3）\"\"\"\n","        conv_3_3_layer = 14  # 选择到第 14 层\n","        cnn = models.vgg19(pretrained=True).features\n","        # 把网络移到正确设备\n","        cnn = cnn.to(self.device)\n","        # 创建一个新的子网络，包含从第 0 层到第 conv_3_3_layer 层\n","        model = nn.Sequential()\n","        model = model.to(self.device)\n","        model = model.eval()\n","        for i, layer in enumerate(list(cnn)):\n","            model.add_module(str(i), layer)\n","            if i == conv_3_3_layer:\n","                break\n","        return model\n","\n","    def initialize(self, loss_fn):\n","        # loss_fn 是一个类（例如 nn.MSELoss()、nn.L1Loss() 等）\n","        # 这里禁止梯度计算\n","        with torch.no_grad():\n","            self.criterion = loss_fn\n","            # 用 builder 方法来得到 contentFunc 网络\n","            self.contentFunc = self.contentFunc_builder()\n","            # transform 已在 __init__ 中初始化\n","\n","    def get_loss(self, fakeIm, realIm):\n","        # 假设 fakeIm, realIm 在调用前还未做归一化或放置 device\n","        # 首先把它们移到同一个 device\n","        fakeIm = fakeIm.to(self.device)\n","        realIm = realIm.to(self.device)\n","\n","        # 假定输入范围是 [-1, 1]，先映射到 [0,1]\n","        fake = (fakeIm + 1.0) / 2.0\n","        real = (realIm + 1.0) / 2.0\n","\n","        # 对第一张样本做 transform（归一化）——这里只对 batch 第0 张处理，其它张你可能也要类似做\n","        # 注意：transform 是对单张图像 (C, H, W) 的操作\n","        fake[0] = self.transform(fake[0])\n","        real[0] = self.transform(real[0])\n","\n","        # 提取特征\n","        f_fake = self.contentFunc(fake)\n","        f_real = self.contentFunc(real)\n","        f_real_detach = f_real.detach()\n","\n","        # 计算感知损失\n","        loss_content = self.criterion(f_fake, f_real_detach)\n","\n","        # 再加上像素级损失：MSE(fake, real)\n","        pixel_loss = nn.MSELoss()(fake, real)\n","\n","        # 返回加权的总损失（你原来是 0.006 * content + 0.5 * mse）\n","        return 0.006 * torch.mean(loss_content) + 0.5 * pixel_loss\n","\n","    def forward(self, fakeIm, realIm):\n","        return self.get_loss(fakeIm, realIm)\n","\n","\n","class GANLoss(nn.Module):\n","    def __init__(self, use_l1=True, target_real_label=1.0, target_fake_label=0.0,\n","                 tensor=torch.FloatTensor):\n","        super(GANLoss, self).__init__()\n","        self.real_label = target_real_label\n","        self.fake_label = target_fake_label\n","        self.real_label_var = None\n","        self.fake_label_var = None\n","        self.Tensor = tensor\n","        if use_l1:\n","            self.loss = nn.L1Loss()\n","        else:\n","            self.loss = nn.BCEWithLogitsLoss()\n","\n","    def get_target_tensor(self, input, target_is_real):\n","        if target_is_real:\n","            create_label = ((self.real_label_var is None) or\n","                            (self.real_label_var.numel() != input.numel()))\n","            if create_label:\n","                real_tensor = self.Tensor(input.size()).fill_(self.real_label)\n","                self.real_label_var = Variable(real_tensor, requires_grad=False)\n","            target_tensor = self.real_label_var\n","        else:\n","            create_label = ((self.fake_label_var is None) or\n","                            (self.fake_label_var.numel() != input.numel()))\n","            if create_label:\n","                fake_tensor = self.Tensor(input.size()).fill_(self.fake_label)\n","                self.fake_label_var = Variable(fake_tensor, requires_grad=False)\n","            target_tensor = self.fake_label_var\n","        return target_tensor.cuda()\n","\n","    def __call__(self, input, target_is_real):\n","        target_tensor = self.get_target_tensor(input, target_is_real)\n","        return self.loss(input, target_tensor)\n","\n","\n","class DiscLoss(nn.Module):\n","    def name(self):\n","        return 'DiscLoss'\n","\n","    def __init__(self):\n","        super(DiscLoss, self).__init__()\n","\n","        self.criterionGAN = GANLoss(use_l1=False)\n","        self.fake_AB_pool = ImagePool(50)\n","\n","    def get_g_loss(self, net, fakeB, realB):\n","        # First, G(A) should fake the discriminator\n","        pred_fake = net.forward(fakeB)\n","        return self.criterionGAN(pred_fake, 1)\n","\n","    def get_loss(self, net, fakeB, realB):\n","        # Fake\n","        # stop backprop to the generator by detaching fake_B\n","        # Generated Image Disc Output should be close to zero\n","        self.pred_fake = net.forward(fakeB.detach())\n","        self.loss_D_fake = self.criterionGAN(self.pred_fake, 0)\n","\n","        # Real\n","        self.pred_real = net.forward(realB)\n","        self.loss_D_real = self.criterionGAN(self.pred_real, 1)\n","\n","        # Combined loss\n","        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n","        return self.loss_D\n","\n","    def __call__(self, net, fakeB, realB):\n","        return self.get_loss(net, fakeB, realB)\n","\n","\n","class RelativisticDiscLoss(nn.Module):\n","    def name(self):\n","        return 'RelativisticDiscLoss'\n","\n","    def __init__(self):\n","        super(RelativisticDiscLoss, self).__init__()\n","\n","        self.criterionGAN = GANLoss(use_l1=False)\n","        self.fake_pool = ImagePool(50)  # create image buffer to store previously generated images\n","        self.real_pool = ImagePool(50)\n","\n","    def get_g_loss(self, net, fakeB, realB):\n","        # First, G(A) should fake the discriminator\n","        self.pred_fake = net.forward(fakeB)\n","\n","        # Real\n","        self.pred_real = net.forward(realB)\n","        errG = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 0) +\n","                self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 1)) / 2\n","        return errG\n","\n","    def get_loss(self, net, fakeB, realB):\n","        # Fake\n","        # stop backprop to the generator by detaching fake_B\n","        # Generated Image Disc Output should be close to zero\n","        self.fake_B = fakeB.detach()\n","        self.real_B = realB\n","        self.pred_fake = net.forward(fakeB.detach())\n","        self.fake_pool.add(self.pred_fake)\n","\n","        # Real\n","        self.pred_real = net.forward(realB)\n","        self.real_pool.add(self.pred_real)\n","\n","        # Combined loss\n","        self.loss_D = (self.criterionGAN(self.pred_real - torch.mean(self.fake_pool.query()), 1) +\n","                       self.criterionGAN(self.pred_fake - torch.mean(self.real_pool.query()), 0)) / 2\n","        return self.loss_D\n","\n","    def __call__(self, net, fakeB, realB):\n","        return self.get_loss(net, fakeB, realB)\n","\n","class DiscLossLS(DiscLoss):\n","    def name(self):\n","        return 'DiscLossLS'\n","\n","    def __init__(self):\n","        super(DiscLossLS, self).__init__()\n","        self.criterionGAN = GANLoss(use_l1=True)\n","\n","    def get_g_loss(self, net, fakeB, realB):\n","        return DiscLoss.get_g_loss(self, net, fakeB)\n","\n","    def get_loss(self, net, fakeB, realB):\n","        return DiscLoss.get_loss(self, net, fakeB, realB)\n","\n","\n","class DiscLossWGANGP(DiscLossLS):\n","    def name(self):\n","        return 'DiscLossWGAN-GP'\n","\n","    def __init__(self):\n","        super(DiscLossWGANGP, self).__init__()\n","        self.LAMBDA = 10\n","\n","    def get_g_loss(self, net, fakeB, realB):\n","        # First, G(A) should fake the discriminator\n","        self.D_fake = net.forward(fakeB)\n","        return -self.D_fake.mean()\n","\n","    def calc_gradient_penalty(self, netD, real_data, fake_data):\n","        alpha = torch.rand(1, 1)\n","        alpha = alpha.expand(real_data.size())\n","        alpha = alpha.cuda()\n","\n","        interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","\n","        interpolates = interpolates.cuda()\n","        interpolates = Variable(interpolates, requires_grad=True)\n","\n","        disc_interpolates = netD.forward(interpolates)\n","\n","        gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                                  grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n","                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n","\n","        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * self.LAMBDA\n","        return gradient_penalty\n","\n","    def get_loss(self, net, fakeB, realB):\n","        self.D_fake = net.forward(fakeB.detach())\n","        self.D_fake = self.D_fake.mean()\n","\n","        # Real\n","        self.D_real = net.forward(realB)\n","        self.D_real = self.D_real.mean()\n","        # Combined loss\n","        self.loss_D = self.D_fake - self.D_real\n","        gradient_penalty = self.calc_gradient_penalty(net, realB.data, fakeB.data)\n","        return self.loss_D + gradient_penalty\n","\n","\n","def get_loss(model):\n","    if model['content_loss'] == 'perceptual':\n","        content_loss = PerceptualLoss()\n","        content_loss.initialize(nn.MSELoss())\n","    elif model['content_loss'] == 'l1':\n","        content_loss = ContentLoss()\n","        content_loss.initialize(nn.L1Loss())\n","    else:\n","        raise ValueError(\"ContentLoss [%s] not recognized.\" % model['content_loss'])\n","\n","    if model['disc_loss'] == 'wgan-gp':\n","        disc_loss = DiscLossWGANGP()\n","    elif model['disc_loss'] == 'lsgan':\n","        disc_loss = DiscLossLS()\n","    elif model['disc_loss'] == 'gan':\n","        disc_loss = DiscLoss()\n","    elif model['disc_loss'] == 'ragan':\n","        disc_loss = RelativisticDiscLoss()\n","    else:\n","        raise ValueError(\"GAN Loss [%s] not recognized.\" % model['disc_loss'])\n","    return content_loss, disc_loss"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"LS2T_3pqJuu_","executionInfo":{"status":"ok","timestamp":1761037109464,"user_tz":-480,"elapsed":16,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["class RelativisticAverageLSGAN(nn.Module):\n","    def __init__(self):\n","        super(RelativisticAverageLSGAN, self).__init__()\n","        self.criterion = nn.MSELoss()\n","\n","    def D_loss(self, pred_real, pred_fake):\n","        real_loss = self.criterion(pred_real - pred_fake.mean(), torch.ones_like(pred_real))\n","        fake_loss = self.criterion(pred_fake - pred_real.mean(), torch.zeros_like(pred_fake))\n","        return 0.5 * (real_loss + fake_loss)\n","\n","    def G_loss(self, pred_real, pred_fake):\n","        real_loss = self.criterion(pred_real - pred_fake.mean(), torch.zeros_like(pred_real))\n","        fake_loss = self.criterion(pred_fake - pred_real.mean(), torch.ones_like(pred_fake))\n","        return 0.5 * (real_loss + fake_loss)"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"itaqiP-mqzhu","executionInfo":{"status":"ok","timestamp":1761037111263,"user_tz":-480,"elapsed":13,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["def relativistic_loss_D(real_pred, fake_pred):\n","    # real_pred, fake_pred 為 Discriminator 判別輸出\n","    # 這是 D 的損失：希望 D(real, fake) 大於 D(fake, real)\n","    # 此為 LSGAN 形式\n","    # L_D = 0.5 * ( (real_pred - torch.mean(fake_pred) - 1)**2 + (fake_pred - torch.mean(real_pred) + 1)**2 )\n","    loss_real = torch.mean((real_pred - torch.mean(fake_pred) - 1) ** 2)\n","    loss_fake = torch.mean((fake_pred - torch.mean(real_pred) + 1) ** 2)\n","    return 0.5 * (loss_real + loss_fake)\n","\n","def relativistic_loss_G(real_pred, fake_pred):\n","    # G 的損失：希望 fake 更真實\n","    # L_G = 0.5 * ( (real_pred - torch.mean(fake_pred) + 1)**2 + (fake_pred - torch.mean(real_pred) - 1)**2 )\n","    loss_real = torch.mean((real_pred - torch.mean(fake_pred) + 1) ** 2)\n","    loss_fake = torch.mean((fake_pred - torch.mean(real_pred) - 1) ** 2)\n","    return 0.5 * (loss_real + loss_fake)"]},{"cell_type":"markdown","metadata":{"id":"LqS98EpjEMGv"},"source":["## PSNR/ SSIM Function"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"6KTtvpDhEGsf","executionInfo":{"status":"ok","timestamp":1761037113660,"user_tz":-480,"elapsed":8,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from math import log10\n","from pytorch_msssim import ssim as ssim_fn  # pip install pytorch-msssim\n","\n","def compute_psnr(fake, real):\n","    \"\"\"Peak Signal-to-Noise Ratio\"\"\"\n","    mse = F.mse_loss(fake, real)\n","    if mse == 0:\n","        return 100\n","    return 10 * log10(1.0 / mse.item())\n","\n","def compute_ssim(fake, real):\n","    \"\"\"Structural Similarity Index\"\"\"\n","    # pytorch_msssim expects (B,C,H,W), value range [0,1]\n","    return ssim_fn(fake, real, data_range=1.0, size_average=True).item()\n"]},{"cell_type":"markdown","metadata":{"id":"umDW57d1SxAz"},"source":["## Train"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RPZR-dUirI0u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761037136275,"user_tz":-480,"elapsed":18235,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}},"outputId":"82a04a34-f8cb-477a-8785-94053f6fe233"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth\" to /root/.cache/torch/hub/checkpoints/inceptionresnetv2-520b38e4.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 213M/213M [00:15<00:00, 14.6MB/s]\n"]}],"source":["G = FPNInception( norm_layer=nn.InstanceNorm2d ).to(device)\n","# 全图鉴别\n","D_global = NLayerDiscriminator(\n","    input_nc=3, ndf=64, n_layers=5,\n","    norm_layer=nn.InstanceNorm2d,\n","    use_sigmoid=False\n",").to(device)\n","\n","# 局部 patch 鉴别\n","D_local = NLayerDiscriminator(\n","    input_nc=3, ndf=64, n_layers=5,\n","    norm_layer=nn.InstanceNorm2d,\n","    use_sigmoid=False\n",").to(device)\n","\n","opt_G = optim.Adam( G.parameters(), lr=5e-5, betas=(0.5, 0.999))\n","opt_D = optim.Adam( list(D_global.parameters()) + list(D_local.parameters()), lr=5e-5, betas=(0.5, 0.999))"]},{"cell_type":"markdown","metadata":{"id":"AwQ1gmqlO9QU"},"source":["### 檢查Checkpoint .pth"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_uPV0U2ZPLzh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761037146253,"user_tz":-480,"elapsed":9973,"user":{"displayName":"Rosalind","userId":"11181186080888946070"}},"outputId":"47b01bb7-2bff-49eb-af21-22b732a94baa"},"outputs":[{"output_type":"stream","name":"stdout","text":["🔄 Loading checkpoint from /content/drive/MyDrive/Colab Notebooks/ModelClearify/checkpoints/deblurgan_v2_latest.pth\n","✅ Resume training from epoch 74\n"]}],"source":["ckpt_path = os.path.join(checkpoint_dir, \"deblurgan_v2_latest.pth\")\n","if os.path.exists(ckpt_path):\n","    print(f\"🔄 Loading checkpoint from {ckpt_path}\")\n","    ckpt = torch.load(ckpt_path, map_location=device)\n","    start_epoch = ckpt[\"epoch\"] + 1\n","    print(f\"✅ Resume training from epoch {start_epoch}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eaN741gH65n6","outputId":"6e5a42ca-2561-44bb-9fb6-b9eff3ae2858"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]},{"output_type":"stream","name":"stdout","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 548M/548M [00:06<00:00, 85.4MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["🔄 Loading checkpoint from /content/drive/MyDrive/Colab Notebooks/ModelClearify/checkpoints/deblurgan_v2_latest.pth\n","✅ Resume training from epoch 74\n","🔄 Loading checkpoint from /content/drive/MyDrive/Colab Notebooks/ModelClearify/checkpoints/metrics_latest.pth\n","Loaded metrics — 74 epochs\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 75/75:  78%|███████▊  | 1349/1740 [31:04<08:15,  1.27s/it, Loss_G=0.0593, Loss_D=0.2528, PSNR=23.33, SSIM=0.776]"]}],"source":["\n","# =========================================================\n","# 4️⃣ Checkpoint 設定\n","# =========================================================\n","ckpt_dir = checkpoint_dir\n","os.makedirs(ckpt_dir, exist_ok=True)\n","start_epoch = 0\n","\n","adversarial_loss = RelativisticAverageLSGAN().to(device)\n","perceptual_loss = PerceptualLoss().to(device)\n","perceptual_loss.initialize(nn.L1Loss())\n","pixel_loss = nn.L1Loss()\n","\n","# 如果有 checkpoint 就自動載入\n","ckpt_path = os.path.join(ckpt_dir, \"deblurgan_v2_latest.pth\")\n","if os.path.exists(ckpt_path):\n","    print(f\"🔄 Loading checkpoint from {ckpt_path}\")\n","    ckpt = torch.load(ckpt_path, map_location=device)\n","    G.load_state_dict(ckpt[\"G\"])\n","    D_global.load_state_dict(ckpt[\"D_global\"])\n","    D_local.load_state_dict(ckpt[\"D_local\"])\n","    opt_G.load_state_dict(ckpt[\"optim_G\"])\n","    opt_D.load_state_dict(ckpt[\"optim_D\"])\n","    start_epoch = ckpt[\"epoch\"] + 1\n","    print(f\"✅ Resume training from epoch {start_epoch}\")\n","\n","g_losses = []\n","d_losses = []\n","psnr_list = []\n","ssim_list = []\n","\n","metrics_path = os.path.join(ckpt_dir, \"metrics_latest.pth\")\n","if os.path.exists(metrics_path):\n","    print(f\"🔄 Loading checkpoint from {metrics_path}\")\n","    metrics_pt = torch.load(metrics_path, map_location=device)\n","    g_losses = metrics_pt[\"g_losses\"]\n","    d_losses = metrics_pt[\"d_losses\"]\n","    psnr_list = metrics_pt[\"psnr_list\"]\n","    ssim_list = metrics_pt[\"ssim_list\"]\n","    print(f\"Loaded metrics — {len(g_losses)} epochs\")\n","\n","# -----------------------------\n","# 1) 定義官方預設的 λ 值\n","# -----------------------------\n","lambda_L1    = 1.5    # L1 重建 loss 權重\n","#lambda_perc  = 0.05      # perceptual loss 權重;default\n","lambda_perc  = 0.10      # perceptual loss 權重;default\n","#lambda_gan   = 5e-3     # adv loss 權重\n","lambda_gan   = 1e-4     # adv loss 權重\n","\n","patch_size = 128  # 可根据图像大小调整\n","num_epochs = 75\n","for epoch in range(start_epoch, num_epochs):\n","  G.train()\n","  D_global.train()\n","  D_local.train()\n","\n","  running_g_loss = 0.0\n","  running_d_loss = 0.0\n","  running_ssim = 0.0\n","  running_psnr = 0.0\n","  pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n","\n","  for i, (blur_img, sharp_img, _) in enumerate(pbar):\n","    blur_img = blur_img.to(device)\n","    sharp_img = sharp_img.to(device)\n","\n","    # ---------------------\n","    # 1) 训练 Discriminator (Global + Local)\n","    # ---------------------\n","    opt_D.zero_grad()\n","    with torch.no_grad():\n","        fake_img = G(blur_img)\n","\n","    # 1.1 全图鉴别\n","    pred_real_g = D_global(sharp_img)\n","    pred_fake_g = D_global(fake_img)\n","\n","    # 1.2 随机裁切同一个位置的局部 patch\n","    B, C, H, W = sharp_img.shape\n","    i = torch.randint(0, H - patch_size + 1, (1,)).item()\n","    j = torch.randint(0, W - patch_size + 1, (1,)).item()\n","    real_patch = sharp_img[:, :, i:i+patch_size, j:j+patch_size]\n","    fake_patch = fake_img[:, :, i:i+patch_size, j:j+patch_size]\n","    pred_real_l = D_local(real_patch)\n","    pred_fake_l = D_local(fake_patch)\n","\n","    # 1.3 计算两个 Discriminator 的 Relativistic loss 并求平均\n","    loss_dg = adversarial_loss.D_loss(pred_real_g, pred_fake_g)\n","    loss_dl = adversarial_loss.D_loss(pred_real_l, pred_fake_l)\n","    loss_d  = 0.5 * (loss_dg + loss_dl)\n","\n","    loss_d.backward()\n","    opt_D.step()\n","\n","    # ---------------------\n","    # 2) 训练 Generator\n","    # ---------------------\n","    opt_G.zero_grad()\n","    fake_img = G(blur_img)\n","\n","    # 2.1 重新计算 Global baseline（detach）与 fake\n","    pred_real_g = D_global(sharp_img).detach()\n","    pred_fake_g = D_global(fake_img)\n","    loss_g_g   = adversarial_loss.G_loss(pred_real_g, pred_fake_g)\n","\n","    # 2.2 重新裁切同样的位置，计算 Local baseline 与 fake\n","    real_patch = sharp_img[:, :, i:i+patch_size, j:j+patch_size]\n","    fake_patch = fake_img[:, :, i:i+patch_size, j:j+patch_size]\n","    pred_real_l = D_local(real_patch).detach()\n","    pred_fake_l = D_local(fake_patch)\n","    loss_g_l    = adversarial_loss.G_loss(pred_real_l, pred_fake_l)\n","\n","    # 2.3 平均两路 adversarial loss\n","    loss_g_adv = 0.5 * (loss_g_g + loss_g_l)\n","\n","    # 2.4 Content Loss\n","    loss_l1   = pixel_loss(fake_img, sharp_img)\n","    loss_perc = perceptual_loss(fake_img, sharp_img)\n","\n","    # 2.5 总 Generator Loss\n","    loss_g = lambda_gan  * loss_g_adv + lambda_L1   * loss_l1 + lambda_perc * loss_perc\n","\n","    loss_g.backward()\n","    opt_G.step()\n","\n","    # ---------------------\n","    # Metrics\n","    # ---------------------\n","    psnr_val = compute_psnr(fake_img.detach(), sharp_img)\n","    ssim_val = compute_ssim(fake_img.detach(), sharp_img)\n","\n","    running_psnr += psnr_val\n","    running_ssim += ssim_val\n","\n","    psnr_list.append(psnr_val)\n","    ssim_list.append(ssim_val)\n","\n","    # ---------------------\n","    # 更新 Running Loss\n","    # ---------------------\n","    running_g_loss += loss_g.item()\n","    running_d_loss += loss_d.item()\n","    pbar.set_postfix({\n","        \"Loss_G\": f\"{loss_g.item():.4f}\",\n","        \"Loss_D\": f\"{loss_d.item():.4f}\",\n","        \"PSNR\": f\"{psnr_val:.2f}\",\n","        \"SSIM\": f\"{ssim_val:.3f}\"\n","    })\n","\n","  # 平均 Loss\n","  avg_g = running_g_loss / len(loader)\n","  avg_d = running_d_loss / len(loader)\n","\n","  avg_psnr = running_psnr / len(loader)\n","  avg_ssim = running_ssim / len(loader)\n","  print(f\"[Epoch {epoch+1}] PSNR: {avg_psnr:.2f} dB | SSIM: {avg_ssim:.4f}\")\n","\n","  g_losses.append(avg_g)\n","  d_losses.append(avg_d)\n","\n","  # -----------------------------\n","  # Save checkpoint\n","  # -----------------------------\n","  torch.save({\n","      \"epoch\": epoch,\n","      \"G\": G.state_dict(),\n","      \"D_global\": D_global.state_dict(),\n","      \"D_local\": D_local.state_dict(),\n","      \"optim_G\": opt_G.state_dict(),\n","      \"optim_D\": opt_D.state_dict()\n","  }, ckpt_path)\n","  print(f\"✅ Checkpoint saved to {ckpt_path}\")\n","\n","  torch.save({\n","      \"g_losses\": g_losses,\n","      \"d_losses\": d_losses,\n","      \"psnr_list\": psnr_list,\n","      \"ssim_list\": ssim_list\n","  }, metrics_path)\n","  print(f\"✅ Loss Metrics saved to {metrics_path}\")\n","\n","print(\"🎉 Training finished!\")\n","# -----------------------------\n","# 可視化 Loss 曲線\n","# -----------------------------\n","plt.figure(figsize=(8,4))\n","plt.plot(g_losses, label='Generator Loss')\n","plt.plot(d_losses, label='Discriminator Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.legend()\n","plt.show()\n","\n","# ==============================\n","# 2️⃣ PSNR / SSIM 曲線\n","# ==============================\n","plt.figure(figsize=(8,4))\n","plt.plot(psnr_list, label='PSNR (dB)')\n","plt.plot(ssim_list, label='SSIM')\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('Image Quality Metrics')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"xBXY55_m_o2a"},"source":["## Load Loss Metric\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qCTCwTZa_ywR"},"outputs":[],"source":["metrics_path = checkpoint_dir + \"/metrics_latest.pth\"\n","metrics_pt = torch.load(metrics_path, map_location=device)\n","latest_g_losses = metrics_pt[\"g_losses\"]\n","latest_d_losses = metrics_pt[\"d_losses\"]\n","latest_psnr_list = metrics_pt[\"psnr_list\"]\n","latest_ssim_list = metrics_pt[\"ssim_list\"]\n","\n","print( len(latest_g_losses) )\n","print( len(latest_g_losses) )\n","\n","# ----------------------------\n","# 1️⃣ Loss 曲線\n","# ----------------------------\n","plt.figure(figsize=(8,4))\n","plt.plot(latest_g_losses, label='Generator Loss', color='tab:red')\n","plt.plot(latest_d_losses, label='Discriminator Loss', color='tab:orange')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title('Training Loss')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()\n","\n","# ----------------------------\n","# 2️⃣ PSNR / SSIM 曲線\n","# ----------------------------\n","plt.figure(figsize=(8,4))\n","plt.plot(latest_psnr_list, label='PSNR (dB)', color='tab:blue')\n","plt.plot(latest_ssim_list, label='SSIM', color='tab:green')\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.title('Image Quality Metrics')\n","plt.legend()\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"pzweLgBuDpvS"},"source":["#### G, D Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nBDDz4L2DojL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761025382497,"user_tz":-480,"elapsed":412,"user":{"displayName":"Clay Chuang","userId":"01347814324055461387"}},"outputId":"3bc7b5e4-7faf-458d-c188-c1c96a4eb809"},"outputs":[{"output_type":"stream","name":"stdout","text":["   Epoch    G_loss    D_loss\n","0     58  0.056471  0.260630\n","1     59  0.056335  0.260417\n","2     60  0.056141  0.259855\n","3     61  0.056089  0.259934\n","4     62  0.055958  0.259817\n","5     63  0.055303  0.259405\n","6     64  0.055485  0.259184\n","7     65  0.055228  0.259763\n","8     66  0.055333  0.258761\n","9     67  0.054954  0.259109\n"]}],"source":["import pandas as pd\n","\n","num_epochs = 10  # 最後 10 個 epoch\n","\n","# 直接取最後 10 筆\n","g_last_10 = latest_g_losses[-num_epochs:]\n","d_last_10 = latest_d_losses[-num_epochs:]\n","\n","# 對應的 epoch 編號（假設從 1 開始）\n","total_epochs = len(latest_g_losses)\n","epoch_numbers = list(range(total_epochs - num_epochs + 1, total_epochs + 1))\n","\n","# 建立表格\n","df_gd = pd.DataFrame({\n","    \"Epoch\": epoch_numbers,\n","    \"G_loss\": g_last_10,\n","    \"D_loss\": d_last_10\n","})\n","\n","print(df_gd)"]},{"cell_type":"markdown","metadata":{"id":"wE0suELzD3ky"},"source":["#### SSIM, PSNR Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NuhhOvrwD677"},"outputs":[],"source":["import numpy as np\n","\n","# 每個 epoch 的資料筆數\n","epoch_size = 1740\n","num_epochs = len(latest_g_losses)\n","\n","# 計算總共多少筆資料\n","total_samples = len(latest_ssim_list)\n","print(\"total_samples :\", total_samples)\n","\n","# 計算最後 10 個 epoch 的起始 index\n","start_index = max(0, total_samples - epoch_size * num_epochs)\n","print(\"start_index :\", start_index)\n","\n","# 取最後 10 個 epoch 的資料\n","last_ssim = latest_ssim_list[start_index:]\n","last_psnr = latest_psnr_list[start_index:]\n","\n","# 分成 10 個 epoch\n","ssim_epochs = np.split(np.array(last_ssim), num_epochs)\n","psnr_epochs = np.split(np.array(last_psnr), num_epochs)\n","\n","# 計算每個 epoch 的平均值\n","ssim_means = [epoch.mean() for epoch in ssim_epochs]\n","psnr_means = [epoch.mean() for epoch in psnr_epochs]\n","\n","# 建立表格\n","import pandas as pd\n","df = pd.DataFrame({\n","    \"Epoch\": [f\"{num_epochs - i}\" for i in range(num_epochs, 0, -1)],\n","    \"PSNR_mean\": psnr_means,\n","    \"SSIM_mean\": ssim_means\n","})\n","\n","print(df)\n","plt.figure(figsize=(10, 6))\n","\n","# 繪製 PSNR 折線圖\n","plt.plot(df[\"Epoch\"], df[\"PSNR_mean\"], marker=\"o\", color=\"tab:blue\", label=\"PSNR Mean\")\n","\n","# 繪製 SSIM 折線圖\n","plt.plot(df[\"Epoch\"], df[\"SSIM_mean\"], marker=\"s\", color=\"tab:orange\", label=\"SSIM Mean\")\n","\n","# 加標題與軸標\n","plt.title(\"PSNR & SSIM (Last 10 Epochs)\", fontsize=14)\n","plt.xlabel(\"Epoch\", fontsize=12)\n","plt.ylabel(\"Value\", fontsize=12)\n","\n","# X軸文字旋轉，避免擠在一起\n","plt.xticks(rotation=45)\n","\n","# 顯示格線與圖例\n","plt.grid(True, linestyle=\"--\", alpha=0.5)\n","plt.legend()\n","\n","# 緊湊排版\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5M2g9U4pEEyC"},"outputs":[],"source":["fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","color = 'tab:blue'\n","ax1.set_xlabel('Epoch')\n","ax1.set_ylabel('PSNR', color=color)\n","ax1.plot(df[\"Epoch\"], df[\"PSNR_mean\"], marker='o', color=color, label='PSNR Mean')\n","ax1.tick_params(axis='y', labelcolor=color)\n","\n","# 第二條 Y 軸\n","ax2 = ax1.twinx()\n","color = 'tab:orange'\n","ax2.set_ylabel('SSIM', color=color)\n","ax2.plot(df[\"Epoch\"], df[\"SSIM_mean\"], marker='s', color=color, label='SSIM Mean')\n","ax2.tick_params(axis='y', labelcolor=color)\n","\n","plt.title(\"PSNR & SSIM (Last Epochs)\")\n","fig.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"w7Eu0Utb5eF5"},"source":["## Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I0EDkA-b5j5h"},"outputs":[],"source":["import torch\n","from PIL import Image\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import os\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 1️⃣ 初始化模型架構（與訓練時相同）\n","G = FPNInception( norm_layer=nn.InstanceNorm2d ).to(device)\n","\n","# 2️⃣ 載入 checkpoint\n","ckpt_path = checkpoint_dir + \"/deblurgan_v2_latest.pth\"\n","\n","checkpoint = torch.load(ckpt_path, map_location=device)\n","missing, unexpected = G.load_state_dict(checkpoint[\"G\"], strict=False)\n","\n","#G.eval()  # 推論模式\n","print(\"✅ Model loaded from\", ckpt_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLEJ-9sCgvZu"},"outputs":[],"source":["def deblur_image_tiled(model, img, device, tile_size=512, overlap=32):\n","    \"\"\"\n","    用 tile-based 方法在 GPU 記憶體有限時推論整張大圖。\n","    Args:\n","        model: 已載入權重的 DeblurGAN-v2 Generator\n","        img: 要處理的影像\n","        device: torch.device(\"cuda\" or \"cpu\")\n","        tile_size: 每塊大小（建議 512）\n","        overlap: 重疊區域像素數（建議 16~64）\n","    \"\"\"\n","    model.eval() # 推論模式\n","\n","    # ---- 預處理 ----\n","    w, h = img.size\n","\n","    # 確保為 32 倍數\n","    new_w = (w // 32) * 32\n","    new_h = (h // 32) * 32\n","    if new_w != w or new_h != h:\n","        img = img.resize((new_w, new_h), Image.BICUBIC)\n","        w, h = new_w, new_h\n","\n","    img_np = np.array(img).astype(np.float32) / 255.0\n","    img_tensor = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).to(device)\n","\n","    # ---- 計算 tile 網格 ----\n","    stride = tile_size - overlap\n","    tiles_x = list(range(0, w, stride))\n","    tiles_y = list(range(0, h, stride))\n","    if tiles_x[-1] + tile_size > w:\n","        tiles_x[-1] = w - tile_size\n","    if tiles_y[-1] + tile_size > h:\n","        tiles_y[-1] = h - tile_size\n","\n","    # ---- 準備空白輸出與權重 ----\n","    output = torch.zeros_like(img_tensor)\n","    weight = torch.zeros_like(img_tensor)\n","\n","    with torch.no_grad():\n","        for y in tiles_y:\n","            for x in tiles_x:\n","                patch = img_tensor[:, :, y:y+tile_size, x:x+tile_size]\n","                pred = model(patch)\n","\n","                # 疊加到對應位置\n","                output[:, :, y:y+tile_size, x:x+tile_size] += pred\n","                weight[:, :, y:y+tile_size, x:x+tile_size] += 1.0\n","\n","    # ---- 平均化（避免重疊區域過曝）----\n","    output /= weight\n","    output = torch.clamp(output, 0, 1)\n","\n","    # ---- 轉回圖片 ----\n","    out_np = (output.squeeze().permute(1, 2, 0).cpu().numpy() * 255.0).astype(np.uint8)\n","    out_img = Image.fromarray(out_np)\n","    return out_img"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o2NaFw3l8A4R"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np\n","\n","# 讀取要超解析的圖片\n","input_path = motion_dir + \"/000_GOPR0372_07_00_000047.png\"  # 你自己的路徑\n","img = Image.open(input_path).convert(\"RGB\")\n","\n","out_img = deblur_image_tiled( G, img, device, tile_size=512, overlap=32)\n","\n","# ============\n","# 4️⃣ 顯示結果\n","# ============\n","plt.figure(figsize=(16,8))\n","plt.subplot(1,2,1)\n","plt.imshow(img)\n","plt.title(\"Input (Blurred)\")\n","plt.axis(\"off\")\n","\n","plt.subplot(1,2,2)\n","plt.imshow(out_img)\n","plt.title(\"Output (Restored by G)\")\n","plt.axis(\"off\")\n","\n","plt.tight_layout()   # 自動調整子圖間距\n","plt.show()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1NwgsDTgMF9LP573nqbeIFT8FPA-KJ-Ca","timestamp":1760837727840},{"file_id":"1ILVBGo6n0L7sKgn02f1l1u9exUI-0J31","timestamp":1760420086042}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}